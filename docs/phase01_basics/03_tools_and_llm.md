# ç¬¬ä¸€é˜¶æ®µï¼šå·¥å…·å’Œ LLM è®²ä¹‰

> å­¦ä¹ æ—¶é—´ï¼š2025-02-08
> çŠ¶æ€ï¼šâœ… å·²æŒæ¡

---

## 1. å®šä¹‰å·¥å…·

### @tool è£…é¥°å™¨

```python
from langchain.tools import tool

@tool
def multiply(a: int, b: int) -> int:
    """ä¹˜æ³•ï¼šè®¡ç®—ä¸¤ä¸ªæ•°çš„ä¹˜ç§¯"""
    return a * b

@tool
def add(a: int, b: int) -> int:
    """åŠ æ³•ï¼šè®¡ç®—ä¸¤ä¸ªæ•°çš„å’Œ"""
    return a + b

# å·¥å…·åˆ—è¡¨
tools = [multiply, add, divide]
```

### å·¥å…·çš„ç»“æ„

```python
@tool
def function_name(param1: type1, param2: type2) -> return_type:
    """å‡½æ•°æ–‡æ¡£å­—ç¬¦ä¸²ï¼ˆLLM ä¼šçœ‹åˆ°è¿™ä¸ªï¼‰"""
    # å‡½æ•°å®ç°
    return result
```

**å…³é”®ç‚¹**ï¼š
- å‡½æ•°åï¼šå·¥å…·çš„åç§°
- ç±»å‹æ³¨è§£ï¼šå‘Šè¯‰ LLM å‚æ•°ç±»å‹
- æ–‡æ¡£å­—ç¬¦ä¸²ï¼šæè¿°å·¥å…·ç”¨é€”ï¼ˆé‡è¦ï¼ï¼‰
- è¿”å›å€¼ï¼šå·¥å…·æ‰§è¡Œç»“æœ

---

## 2. LLM é…ç½®

### OpenAIï¼ˆå®˜æ–¹ï¼‰

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="gpt-4o-mini",
    api_key="sk-...",
    temperature=0
)
```

### Anthropic

```python
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    api_key="sk-ant-...",
    temperature=0
)
```

### Google

```python
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    api_key="...",
    temperature=0
)
```

### ç¬¬ä¸‰æ–¹æä¾›å•†ï¼ˆå¦‚æ™ºè°±ï¼‰

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="glm-4-flash",
    api_key="your-zhipu-api-key",
    base_url="https://open.bigmodel.cn/api/paas/v4/",  # â† å…³é”®ï¼
    temperature=0
)
```

**å…³é”®ç‚¹**ï¼š
- ä½¿ç”¨ `ChatOpenAI`ï¼ˆå…¼å®¹ OpenAI APIï¼‰
- è®¾ç½® `base_url` ä¸ºç¬¬ä¸‰æ–¹ API åœ°å€
- å…¶ä»–å‚æ•°ç›¸åŒ

---

## 3. bind_tools() - ç»‘å®šå·¥å…·

### åŸºç¡€ç”¨æ³•

```python
# åˆ›å»º LLM
llm = ChatOpenAI(...)

# ç»‘å®šå·¥å…·
llm_with_tools = llm.bind_tools(tools)

# ä½¿ç”¨
response = llm_with_tools.invoke(messages)
```

### bind_tools() çš„ä½œç”¨

å‘Šè¯‰ LLM æœ‰å“ªäº›å·¥å…·å¯ç”¨ï¼š

```python
# æ²¡æœ‰ bind_tools
llm.invoke("3ä¹˜ä»¥5")  # LLM åªèƒ½æ–‡å­—å›ç­”

# æœ‰ bind_tools
llm_with_tools.invoke("3ä¹˜ä»¥5")  # LLM å¯èƒ½è¿”å› tool_calls
```

### å·¥ä½œåŸç†

```
LLM + bind_tools(tools)
    â†“
LLM çŸ¥é“å·¥å…·åˆ—è¡¨ï¼š
  - multiply(a, b)
  - add(a, b)
  - divide(a, b)
    â†“
ç”¨æˆ·è¾“å…¥ï¼š"3ä¹˜ä»¥5"
    â†“
LLM è¿”å›ï¼š
AIMessage(
    content="",
    tool_calls=[{
        "name": "multiply",
        "args": {"a": 3, "b": 5},
        "id": "call_001"
    }]
)
```

---

## 4. LLM èŠ‚ç‚¹å®ç°

### å®Œæ•´çš„ LLM èŠ‚ç‚¹

```python
def llm_node(state):
    """
    LLM èŠ‚ç‚¹ï¼šè°ƒç”¨ LLM åšå†³ç­–
    """
    # 1. è·å–æ¶ˆæ¯åˆ—è¡¨
    messages = state["messages"]

    # 2. è°ƒç”¨ LLMï¼ˆå¸¦å·¥å…·ï¼‰
    response = llm_with_tools.invoke(messages)

    # 3. è¿”å›æ›´æ–°çš„æ¶ˆæ¯åˆ—è¡¨
    return {"messages": [response]}
```

### ç®€åŒ–ç‰ˆæœ¬

```python
def llm_node(state):
    response = llm_with_tools.invoke(state["messages"])
    return {"messages": [response]}
```

---

## 5. ToolNode ä½¿ç”¨

### åˆ›å»º ToolNode

```python
from langgraph.prebuilt import ToolNode

tool_node = ToolNode(tools)
```

### ToolNode è‡ªåŠ¨åšä»€ä¹ˆï¼Ÿ

```python
# è¾“å…¥ï¼šState åŒ…å« AIMessageï¼ˆæœ‰ tool_callsï¼‰
state = {
    "messages": [
        AIMessage(
            content="",
            tool_calls=[{
                "name": "multiply",
                "args": {"a": 3, "b": 5},
                "id": "call_001"
            }]
        )
    ]
}

# ToolNode æ‰§è¡Œåè¿”å›ï¼š
result = {
    "messages": [
        ToolMessage(
            content=15,
            tool_call_id="call_001"
        )
    ]
}
```

---

## 6. å®Œæ•´çš„å·¥å…·è°ƒç”¨æµç¨‹

### æµç¨‹å›¾

```
1. ç”¨æˆ·è¾“å…¥
   HumanMessage("3ä¹˜ä»¥5")
       â†“
2. LLM èŠ‚ç‚¹
   llm_with_tools.invoke(messages)
       â†“
3. LLM å†³å®šè°ƒç”¨å·¥å…·
   AIMessage(
       tool_calls=[{"name": "multiply", "args": {"a": 3, "b": 5}, "id": "call_001"}]
   )
       â†“
4. æ¡ä»¶åˆ¤æ–­
   should_continue() æ£€æµ‹åˆ° tool_calls
       â†“
5. ToolNode æ‰§è¡Œ
   multiply(3, 5) â†’ 15
       â†“
6. è¿”å›å·¥å…·ç»“æœ
   ToolMessage(content=15, tool_call_id="call_001")
       â†“
7. å›åˆ° LLM èŠ‚ç‚¹
   LLM çœ‹åˆ°å·¥å…·ç»“æœï¼Œç”Ÿæˆæœ€ç»ˆå›å¤
       â†“
8. LLM å›ç­”
   AIMessage(content="3ä¹˜ä»¥5ç­‰äº15")
       â†“
9. æ¡ä»¶åˆ¤æ–­
   should_continue() æ²¡æœ‰æ£€æµ‹åˆ° tool_calls
       â†“
10. END
```

### ä»£ç å®ç°

```python
from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.prebuilt import ToolNode
from langchain.tools import tool
from langchain_openai import ChatOpenAI

# 1. å®šä¹‰å·¥å…·
@tool
def multiply(a: int, b: int) -> int:
    return a * b

tools = [multiply]

# 2. å®šä¹‰çŠ¶æ€
class CalculatorState(MessagesState):
    pass

# 3. åˆ›å»º LLM å¹¶ç»‘å®šå·¥å…·
llm = ChatOpenAI(model="glm-4-flash", ...)
llm_with_tools = llm.bind_tools(tools)

# 4. å®šä¹‰èŠ‚ç‚¹
def llm_node(state):
    response = llm_with_tools.invoke(state["messages"])
    return {"messages": [response]}

tool_node = ToolNode(tools)

# 5. å®šä¹‰æ¡ä»¶å‡½æ•°
def should_continue(state):
    last_msg = state["messages"][-1]
    if hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
        return "tools"
    return END

# 6. æ„å»ºå›¾
graph = StateGraph(CalculatorState)
graph.add_node("llm", llm_node)
graph.add_node("tools", tool_node)

graph.add_edge(START, "llm")
graph.add_conditional_edges("llm", should_continue)
graph.add_edge("tools", "llm")

# 7. ç¼–è¯‘å¹¶è¿è¡Œ
app = graph.compile()
result = app.invoke({"messages": [HumanMessage("3ä¹˜ä»¥5")]})
```

---

## 7. é…ç½®ç®¡ç†ï¼ˆYAML + Pydanticï¼‰

### é…ç½®æ–‡ä»¶ç»“æ„

```yaml
llm:
  provider: zhipu

  models:
    zhipu:
      model: glm-4-flash
      api_key: your-api-key
      base_url: https://open.bigmodel.cn/api/paas/v4/
      temperature: 0

    openai:
      model: gpt-4o-mini
      api_key: your-openai-key
      temperature: 0
```

### Pydantic æ¨¡å‹

```python
from pydantic import BaseModel
from typing import Optional
import yaml

class LLMModelConfig(BaseModel):
    """LLM æ¨¡å‹é…ç½®"""
    model: str
    api_key: str
    base_url: Optional[str] = None
    temperature: float = 0

class LLMConfig(BaseModel):
    """LLM é…ç½®"""
    provider: str
    models: dict[str, LLMModelConfig]

def load_config(config_path: str = "config.yaml") -> LLMConfig:
    """è¯»å–é…ç½®æ–‡ä»¶"""
    with open(config_path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f)

    llm_data = data["llm"]
    return LLMConfig(**llm_data)
```

### ä½¿ç”¨é…ç½®

```python
# åŠ è½½é…ç½®
llm_config = load_config()

# è·å–å½“å‰æä¾›å•†çš„é…ç½®
llm_model = llm_config.models[llm_config.provider]

# åˆ›å»º LLM
llm = ChatOpenAI(
    model=llm_model.model,
    api_key=llm_model.api_key,
    base_url=llm_model.base_url,
    temperature=llm_model.temperature
)

# ç»‘å®šå·¥å…·
llm_with_tools = llm.bind_tools(tools)
```

---

## 8. å…³é”®æ¦‚å¿µæ€»ç»“

### å·¥å…·è°ƒç”¨ä¸‰è¦ç´ 

1. **å®šä¹‰å·¥å…·** (`@tool`)
2. **ç»‘å®šå·¥å…·** (`bind_tools()`)
3. **æ‰§è¡Œå·¥å…·** (`ToolNode`)

### LLM åœ¨å·¥å…·è°ƒç”¨ä¸­çš„è§’è‰²

```
LLM = å†³ç­–è€…
- åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
- é€‰æ‹©å“ªä¸ªå·¥å…·
- æå–å·¥å…·å‚æ•°

ToolNode = æ‰§è¡Œè€…
- æ‰§è¡Œå·¥å…·
- è¿”å›ç»“æœ
```

---

## 9. å¸¸è§é—®é¢˜

### Q1: LLM ä¸è°ƒç”¨å·¥å…·ï¼Ÿ

**æ£€æŸ¥**ï¼š
- å·¥å…·çš„æ–‡æ¡£å­—ç¬¦ä¸²æ˜¯å¦æ¸…æ™°ï¼Ÿ
- å·¥å…·çš„å‚æ•°ç±»å‹æ³¨è§£æ˜¯å¦æ­£ç¡®ï¼Ÿ
- ç”¨æˆ·è¾“å…¥æ˜¯å¦æ˜ç¡®éœ€è¦å·¥å…·ï¼Ÿ

### Q2: ToolNode æŠ¥é”™ï¼Ÿ

**æ£€æŸ¥**ï¼š
- å·¥å…·åˆ—è¡¨æ˜¯å¦æ­£ç¡®ï¼Ÿ
- `tool_call_id` æ˜¯å¦åŒ¹é…ï¼Ÿ

### Q3: å¦‚ä½•åˆ‡æ¢ LLM æä¾›å•†ï¼Ÿ

**æ–¹æ³•**ï¼šä¿®æ”¹ YAML é…ç½®æ–‡ä»¶
```yaml
llm:
  provider: openai  # ä» zhipu æ”¹ä¸º openai
```

---

## 10. ç»ƒä¹ æ£€æŸ¥æ¸…å•

- [ ] èƒ½ä½¿ç”¨ `@tool` å®šä¹‰å·¥å…·
- [ ] èƒ½é…ç½®ä¸åŒçš„ LLM æä¾›å•†
- [ ] èƒ½ä½¿ç”¨ `bind_tools()` ç»‘å®šå·¥å…·
- [ ] èƒ½å®ç° LLM èŠ‚ç‚¹
- [ ] èƒ½ä½¿ç”¨ ToolNode
- [ ] èƒ½å®ç°æ¡ä»¶å‡½æ•°ï¼ˆshould_continueï¼‰
- [ ] èƒ½æ„å»ºå®Œæ•´çš„å·¥å…·è°ƒç”¨ Agent
- [ ] èƒ½ä½¿ç”¨ YAML + Pydantic ç®¡ç†é…ç½®

---

**ä¸‹ä¸€æ­¥**ï¼šå­¦ä¹  [04_configuration.md](./04_configuration.md) ğŸ“–
