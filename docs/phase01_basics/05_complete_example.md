# ç¬¬ä¸€é˜¶æ®µï¼šå®Œæ•´ç¤ºä¾‹è®²ä¹‰

> å­¦ä¹ æ—¶é—´ï¼š2025-02-08
> çŠ¶æ€ï¼šâœ… å·²æŒæ¡

---

## é¡¹ç›®ï¼šè®¡ç®—å™¨ Agent

**ç›®æ ‡**ï¼šæ„å»ºä¸€ä¸ªèƒ½è¿›è¡ŒåŠ å‡ä¹˜é™¤çš„ AI Agent

---

## å®Œæ•´ä»£ç 

```python
"""
æˆ‘çš„ç¬¬ä¸€ä¸ª AI Agent - è®¡ç®—å™¨
ç›®æ ‡ï¼šç†è§£ LLM å¦‚ä½•è°ƒç”¨å·¥å…·
"""

import sys
import os
if os.name == 'nt':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

from typing import Literal
from typing_extensions import TypedDict

from langchain.tools import tool
from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.prebuilt import ToolNode
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

from pydantic import BaseModel
from typing import Optional
import yaml

# ============ æ­¥éª¤1ï¼šå®šä¹‰å·¥å…· ============
@tool
def multiply(a: int, b: int) -> int:
    """ä¹˜æ³•ï¼šè®¡ç®—ä¸¤ä¸ªæ•°çš„ä¹˜ç§¯"""
    return a * b

@tool
def add(a: int, b: int) -> int:
    """åŠ æ³•ï¼šè®¡ç®—ä¸¤ä¸ªæ•°çš„å’Œ"""
    return a + b

@tool
def divide(a: int, b: int) -> float:
    """é™¤æ³•ï¼šè®¡ç®—ä¸¤ä¸ªæ•°çš„å•†"""
    return a / b

tools = [multiply, add, divide]

# ============ æ­¥éª¤2ï¼šå®šä¹‰çŠ¶æ€ ============
class CalculatorState(MessagesState):
    """è®¡ç®—å™¨çŠ¶æ€ï¼šç»§æ‰¿ MessagesStateï¼Œä½¿ç”¨é»˜è®¤çš„ messages å­—æ®µ"""
    pass

# ============ æ­¥éª¤3ï¼šå®šä¹‰å·¥å…·èŠ‚ç‚¹ ============
tool_node = ToolNode(tools)

# ============ æ­¥éª¤4ï¼šé…ç½® LLM ============
class LLMModelConfig(BaseModel):
    """LLM æ¨¡å‹é…ç½®"""
    model: str
    api_key: str
    base_url: Optional[str] = None
    temperature: float = 0

class LLMConfig(BaseModel):
    """LLM é…ç½®"""
    provider: str
    models: dict[str, LLMModelConfig]

def load_config(config_path: str = "config.yaml") -> LLMConfig:
    """è¯»å–é…ç½®æ–‡ä»¶"""
    with open(config_path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f)

    llm_data = data["llm"]
    return LLMConfig(**llm_data)

# åŠ è½½é…ç½®
llm_config = load_config()
llm_model = llm_config.models[llm_config.provider]

# åˆ›å»º LLM å¹¶ç»‘å®šå·¥å…·
llm = ChatOpenAI(
    model=llm_model.model,
    api_key=llm_model.api_key,
    base_url=llm_model.base_url,
    temperature=llm_model.temperature
)
llm_with_tools = llm.bind_tools(tools)

# ============ æ­¥éª¤5ï¼šå®šä¹‰èŠ‚ç‚¹ ============
def llm_node(state):
    """
    LLM èŠ‚ç‚¹ï¼šè°ƒç”¨ LLM åšå†³ç­–
    """
    response = llm_with_tools.invoke(state["messages"])
    return {"messages": [response]}

def should_continue(state) -> Literal["tools", END]:
    """
    æ¡ä»¶å‡½æ•°ï¼šåˆ¤æ–­æ˜¯å¦ç»§ç»­è°ƒç”¨å·¥å…·
    """
    last_message = state["messages"][-1]

    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    return END

# ============ æ­¥éª¤6ï¼šæ„å»ºå›¾ ============
graph = StateGraph(CalculatorState)

# æ·»åŠ èŠ‚ç‚¹
graph.add_node("llm", llm_node)
graph.add_node("tools", tool_node)

# æ·»åŠ è¾¹
graph.add_edge(START, "llm")
graph.add_conditional_edges("llm", should_continue)
graph.add_edge("tools", "llm")

# ============ æ­¥éª¤7ï¼šç¼–è¯‘å¹¶æµ‹è¯• ============
app = graph.compile()

# æµ‹è¯•
print("\n=== æµ‹è¯•1ï¼š3 ä¹˜ä»¥ 5 ===")
result = app.invoke({"messages": [HumanMessage("3 ä¹˜ä»¥ 5 ç­‰äºå¤šå°‘ï¼Ÿ")]})

print("\næœ€ç»ˆæ¶ˆæ¯åˆ—è¡¨ï¼š")
for i, msg in enumerate(result["messages"]):
    msg_type = type(msg).__name__
    print(f"{i+1}. {msg_type}: {msg.content}")
    if hasattr(msg, "tool_calls"):
        print(f"   å·¥å…·è°ƒç”¨: {msg.tool_calls}")

print("\n=== æµ‹è¯•2ï¼š10 é™¤ä»¥ 2 ===")
result2 = app.invoke({"messages": [HumanMessage("10 é™¤ä»¥ 2 ç­‰äºå¤šå°‘ï¼Ÿ")]})

print("\næœ€ç»ˆæ¶ˆæ¯åˆ—è¡¨ï¼š")
for i, msg in enumerate(result2["messages"]):
    msg_type = type(msg).__name__
    print(f"{i+1}. {msg_type}: {msg.content}")
    if hasattr(msg, "tool_calls"):
        print(f"   å·¥å…·è°ƒç”¨: {msg.tool_calls}")
```

---

## æ‰§è¡Œæµç¨‹è¯¦è§£

### æµ‹è¯•1ï¼š`3 ä¹˜ä»¥ 5`

#### ç¬¬1æ­¥ï¼šç”¨æˆ·è¾“å…¥
```python
HumanMessage(content="3 ä¹˜ä»¥ 5 ç­‰äºå¤šå°‘ï¼Ÿ")
```

#### ç¬¬2æ­¥ï¼šLLM èŠ‚ç‚¹
```python
llm_with_tools.invoke([HumanMessage(...)])
```

**LLM çš„è¾“å…¥**ï¼š
- å·¥å…·åˆ—è¡¨ï¼š`multiply`, `add`, `divide`
- ç”¨æˆ·æ¶ˆæ¯ï¼š"3 ä¹˜ä»¥ 5 ç­‰äºå¤šå°‘ï¼Ÿ"

**LLM çš„è¾“å‡º**ï¼š
```python
AIMessage(
    content="",
    tool_calls=[{
        "name": "multiply",
        "args": {"a": 3, "b": 5},
        "id": "call_001"
    }]
)
```

#### ç¬¬3æ­¥ï¼šæ¡ä»¶åˆ¤æ–­
```python
last_message = state["messages"][-1]
# last_message æ˜¯ AIMessageï¼Œæœ‰ tool_calls
hasattr(last_message, "tool_calls")  # True
last_message.tool_calls              # ä¸ä¸ºç©º
# è¿”å› "tools"
```

#### ç¬¬4æ­¥ï¼šToolNode æ‰§è¡Œ
```python
multiply(a=3, b=5)  # è¿”å› 15
```

**ToolNode çš„è¾“å‡º**ï¼š
```python
ToolMessage(
    content=15,
    tool_call_id="call_001"
)
```

#### ç¬¬5æ­¥ï¼šå›åˆ° LLM èŠ‚ç‚¹
```python
llm_with_tools.invoke([
    HumanMessage("3 ä¹˜ä»¥ 5 ç­‰äºå¤šå°‘ï¼Ÿ"),
    AIMessage(tool_calls=[...]),
    ToolMessage(content=15, tool_call_id="call_001")
])
```

**LLM çœ‹åˆ°å·¥å…·ç»“æœ**ï¼Œç”Ÿæˆæœ€ç»ˆå›å¤ï¼š
```python
AIMessage(content="3 ä¹˜ä»¥ 5 ç­‰äº 15")
```

#### ç¬¬6æ­¥ï¼šæ¡ä»¶åˆ¤æ–­
```python
last_message = state["messages"][-1]
# last_message æ˜¯ AIMessageï¼Œæ²¡æœ‰ tool_calls
hasattr(last_message, "tool_calls")  # True
last_message.tool_calls              # ç©ºåˆ—è¡¨
# è¿”å› END
```

#### ç¬¬7æ­¥ï¼šç»“æŸ

**æœ€ç»ˆæ¶ˆæ¯åˆ—è¡¨**ï¼š
```python
[
    HumanMessage("3 ä¹˜ä»¥ 5 ç­‰äºå¤šå°‘ï¼Ÿ"),
    AIMessage(tool_calls=[...]),
    ToolMessage(content=15),
    AIMessage(content="3 ä¹˜ä»¥ 5 ç­‰äº 15")
]
```

---

## é…ç½®æ–‡ä»¶

### config.yaml

```yaml
llm:
  # é»˜è®¤ä½¿ç”¨çš„ LLM æä¾›å•†
  provider: zhipu

  # å„ä¸ªæä¾›å•†çš„é…ç½®
  models:
    zhipu:
      model: glm-4-flash
      api_key: your-zhipu-api-key
      base_url: https://open.bigmodel.cn/api/paas/v4/
      temperature: 0

    openai:
      model: gpt-4o-mini
      api_key: your-openai-api-key
      temperature: 0
```

---

## å…³é”®çŸ¥è¯†ç‚¹æ€»ç»“

### 1. å·¥å…·è°ƒç”¨

```python
# å®šä¹‰å·¥å…·
@tool
def multiply(a: int, b: int) -> int:
    return a * b

# ç»‘å®šå·¥å…·
llm_with_tools = llm.bind_tools([multiply])

# æ‰§è¡Œå·¥å…·
tool_node = ToolNode([multiply])
```

### 2. çŠ¶æ€ç®¡ç†

```python
# å®šä¹‰çŠ¶æ€
class CalculatorState(MessagesState):
    pass

# è®¿é—®çŠ¶æ€
messages = state["messages"]

# æ›´æ–°çŠ¶æ€
return {"messages": [new_msg]}
```

### 3. æ¡ä»¶è·¯ç”±

```python
# æ¡ä»¶å‡½æ•°
def should_continue(state):
    if has_tool_calls:
        return "tools"
    return END

# æ·»åŠ æ¡ä»¶è¾¹
graph.add_conditional_edges("llm", should_continue)
```

### 4. é…ç½®ç®¡ç†

```python
# Pydantic æ¨¡å‹
class LLMModelConfig(BaseModel):
    model: str
    api_key: str
    base_url: Optional[str] = None

# è¯»å–é…ç½®
config = load_config()
llm = ChatOpenAI(**config.model_dump())
```

---

## æ‰©å±•ç»ƒä¹ 

### ç»ƒä¹ 1ï¼šæ·»åŠ æ›´å¤šå·¥å…·

æ·»åŠ ä¸€ä¸ªå–æ¨¡è¿ç®—ï¼š
```python
@tool
def modulo(a: int, b: int) -> int:
    """å–æ¨¡ï¼šè®¡ç®— a é™¤ä»¥ b çš„ä½™æ•°"""
    return a % b

tools = [multiply, add, divide, modulo]
```

### ç»ƒä¹ 2ï¼šæ·»åŠ çŠ¶æ€å­—æ®µ

```python
class CalculatorState(MessagesState):
    calculation_count: int  # è®¡ç®—æ¬¡æ•°

def llm_node(state):
    response = llm_with_tools.invoke(state["messages"])
    count = state.get("calculation_count", 0) + 1
    return {"messages": [response], "calculation_count": count}
```

### ç»ƒä¹ 3ï¼šåˆ‡æ¢ LLM æä¾›å•†

ä¿®æ”¹ `config.yaml`ï¼š
```yaml
llm:
  provider: openai  # ä» zhipu æ”¹ä¸º openai
```

ä¸éœ€è¦ä¿®æ”¹ä»£ç ï¼

---

## å¸¸è§é—®é¢˜

### Q1: LLM ä¸è°ƒç”¨å·¥å…·ï¼Ÿ

**å¯èƒ½åŸå› **ï¼š
1. å·¥å…·çš„æ–‡æ¡£å­—ç¬¦ä¸²ä¸æ¸…æ™°
2. ç”¨æˆ·è¾“å…¥ä¸æ˜ç¡®
3. LLM é…ç½®é—®é¢˜

**è§£å†³æ–¹æ³•**ï¼š
```python
# æ£€æŸ¥å·¥å…·å®šä¹‰
@tool
def multiply(a: int, b: int) -> int:
    """ä¹˜æ³•ï¼šè®¡ç®—ä¸¤ä¸ªæ•°çš„ä¹˜ç§¯"""  # â† ç¡®ä¿æè¿°æ¸…æ™°
    return a * b

# æµ‹è¯• LLM æ˜¯å¦çŸ¥é“å·¥å…·
print(llm_with_tools.invoke("ä½ æœ‰å“ªäº›å·¥å…·ï¼Ÿ"))
```

### Q2: ToolNode æŠ¥é”™ï¼Ÿ

**å¯èƒ½åŸå› **ï¼š
1. å·¥å…·æ‰§è¡Œå¤±è´¥
2. å‚æ•°ç±»å‹é”™è¯¯

**è§£å†³æ–¹æ³•**ï¼š
```python
# æ·»åŠ é”™è¯¯å¤„ç†
@tool
def divide(a: int, b: int) -> float:
    """é™¤æ³•ï¼šè®¡ç®—ä¸¤ä¸ªæ•°çš„å•†"""
    if b == 0:
        return "é”™è¯¯ï¼šé™¤æ•°ä¸èƒ½ä¸º0"
    return a / b
```

### Q3: å¦‚ä½•è°ƒè¯•ï¼Ÿ

**æ–¹æ³•1ï¼šæ‰“å°æ¶ˆæ¯**
```python
def llm_node(state):
    print(f"LLM èŠ‚ç‚¹è¾“å…¥: {state['messages']}")
    response = llm_with_tools.invoke(state["messages"])
    print(f"LLM èŠ‚ç‚¹è¾“å‡º: {response}")
    return {"messages": [response]}
```

**æ–¹æ³•2ï¼šæŸ¥çœ‹æ¶ˆæ¯åˆ—è¡¨**
```python
result = app.invoke({"messages": [HumanMessage("...")]})

for i, msg in enumerate(result["messages"]):
    print(f"{i+1}. {type(msg).__name__}: {msg.content}")
    if hasattr(msg, "tool_calls"):
        print(f"   å·¥å…·è°ƒç”¨: {msg.tool_calls}")
```

---

## å­¦ä¹ æˆæœæ£€æŸ¥

å®Œæˆè¿™ä¸ªé¡¹ç›®åï¼Œä½ åº”è¯¥ï¼š

- [ ] ç†è§£ LangGraph çš„æ ¸å¿ƒæ¦‚å¿µï¼ˆStateã€Nodesã€Edgesï¼‰
- [ ] èƒ½å®šä¹‰å’Œä½¿ç”¨å·¥å…·ï¼ˆ`@tool`ã€`bind_tools()`ã€`ToolNode`ï¼‰
- [ ] èƒ½æ„å»ºå®Œæ•´çš„å·¥å…·è°ƒç”¨ Agent
- [ ] èƒ½ä½¿ç”¨ Pydantic + YAML ç®¡ç†é…ç½®
- [ ] èƒ½é˜…è¯»å’Œç†è§£æºç 
- [ ] èƒ½è°ƒè¯•å’Œä¼˜åŒ– Agent

---

**æ­å–œä½ å®Œæˆç¬¬ä¸€é˜¶æ®µçš„å­¦ä¹ ï¼** ğŸ‰

ä½ å·²ç»æŒæ¡äº† LangGraph çš„æ ¸å¿ƒæ¦‚å¿µï¼Œå¯ä»¥å¼€å§‹ç¬¬äºŒé˜¶æ®µçš„å­¦ä¹ äº†ï¼

---

**ä¸‹ä¸€æ­¥**ï¼šæ¢ç´¢æ›´å¤æ‚çš„åº”ç”¨åœºæ™¯
- RAG Agent
- å¤š Agent ç³»ç»Ÿ
- äººæœºåä½œï¼ˆHuman-in-the-loopï¼‰
